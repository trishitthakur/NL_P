{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24cf6736",
   "metadata": {},
   "source": [
    "## from nlp specialiazation and coded by trishit nath thakur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faabc05",
   "metadata": {},
   "source": [
    "# logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac590e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from os import getcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc6574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import twitter_samples \n",
    "\n",
    "from utils import process_tweet, build_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c22931",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c38a6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## splitting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e8085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg \n",
    "\n",
    "test_x = test_pos + test_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2313ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(test_pos), 1)), axis = 0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c414566",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create frequency dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b37c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = build_freqs(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85404db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The given function process_tweet() tokenizes the tweet into individual words, removes stop words and applies stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb6ed70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    h = 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c446a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(x, y, theta, alpha, num_iters):\n",
    "    ''' x: matrix of features which is (m,n+1)\n",
    "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
    "        theta: weight vector of dimension (n+1,1)\n",
    "        alpha: learning rate\n",
    "        num_iters: number of iterations you want to train your model for\n",
    "        output: J: the final cost\n",
    "                theta: your final weight vector'''\n",
    "    m = x.shape[0]\n",
    "    \n",
    "    for i in range(o, num_iters):\n",
    "        \n",
    "        z = np.dot(x, theta)\n",
    "        h = sigmoid(h)\n",
    "        \n",
    "        \n",
    "        J = -1./m * (np.dot(y.T, np.log(h)) + np.dot((1-y).T,np.log(1-h)))  \n",
    "        \n",
    "        theta = theta - (alpha/m) * np.dot(x.T,(h-y))\n",
    "        \n",
    "    J = float(J)\n",
    "    return J, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4187fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet, freqs):\n",
    "    \n",
    "    '''Input: \n",
    "        tweet: a list of words for one tweet\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "    Output: \n",
    "        x: a feature vector of dimension (1,3)\n",
    "    '''\n",
    "    \n",
    "    # process_tweet tokenizes, stems, and removes stopwords\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    # 3 elements in the form of a 1 x 3 vector\n",
    "    x = np.zeros((1, 3)) \n",
    "    \n",
    "    # bias term set to 0\n",
    "    x[0,0] = 1\n",
    "    \n",
    "    for word in word_l:\n",
    "        \n",
    "        x[0,1] += freqs.get((word, 1), 0)  # increment the word count for the positive label 1\n",
    "        \n",
    "        x[0,2] += freqs.get((word, 0), 0)  # increment the word count for the negative label 0\n",
    "    \n",
    "    assert(x.shape == (1, 3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51334311",
   "metadata": {},
   "outputs": [],
   "source": [
    "## training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658b21a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(train_x), 3))\n",
    "\n",
    "for i in range(len(train_x)):\n",
    "    X[i, :] = extract_features(train_x[i], freqs)\n",
    "    \n",
    "    \n",
    "Y = train_y\n",
    "\n",
    "J, theta = gradientDescent(X, Y, np.zeros((3, 1), 1e-9, 5000)\n",
    "                \n",
    "print(f\"The cost after training is {J:.8f}.\")\n",
    "                           \n",
    "print(f\"the resulting vector of weights is {[rount(t, 8) for t in np.squeeze(theta)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23361c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac71becf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, freqs, theta):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a string\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "        theta: (3,1) vector of weights\n",
    "    Output: \n",
    "        y_pred: the probability of a tweet being positive or negative\n",
    "    '''\n",
    "    \n",
    "    x = extract_features(tweet,freqs)  # extract the features of the tweet and store it into x\n",
    "    \n",
    "    y_pred = sigmoid(np.dot(x,theta))  # make the prediction using x and theta\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67655444",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.']:\n",
    "    print('%s -> %f'%(tweet, predict_tweets(tweet, freqs, theta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6139a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfc0b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        test_x: a list of tweets\n",
    "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
    "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
    "        theta: weight vector of dimension (3, 1)\n",
    "    Output: \n",
    "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
    "    \"\"\"\n",
    "    \n",
    "    y_hat = []\n",
    "    \n",
    "    for tweet in test_x:\n",
    "        y_pred = predict_tweet(tweet, freqs, theta)\n",
    "        \n",
    "        if y_pred > 0.5:\n",
    "            y_hat.append(1)\n",
    "        else:\n",
    "            y_hat.append(0)\n",
    "    \n",
    "    \n",
    "    accuracy = (y_hat==np.squeeze(test_y)).sum()/len(test_x)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0897699d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
    "print(f\"the accuracy on test set is {tmp_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
