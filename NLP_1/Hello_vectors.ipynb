{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7cb11d0",
   "metadata": {},
   "source": [
    "## Hello vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d037f16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## from the NLP specialiazation and coded by trishit nath thakur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c80882",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing data\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import get_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d7cf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('capitals.txt', delimiter=' ')\n",
    "data.columns = ['city1', 'country1', 'city2', 'country2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6826ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataset imported from https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a42ab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "embeddings = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary = True)\n",
    "\n",
    "f = open('capitals.txt', 'r').read()\n",
    "\n",
    "set_words = set(nltk.word_tokenize(f))\n",
    "\n",
    "select_words = words = ['king', 'queen', 'oil', 'gas', 'happy', 'sad', 'city', 'town', 'village', 'country', 'continent', 'petroleum', 'joyful']\n",
    "for w in select_words:\n",
    "    set_words.add(w)\n",
    "\n",
    "def get_word_embeddings(embeddings):\n",
    "\n",
    "    word_embeddings = {}\n",
    "    for word in embeddings.vocab:\n",
    "        if word in set_words:\n",
    "            word_embeddings[word] = embeddings[word]\n",
    "    return word_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8fb9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = pickle.load(open(\"word_embeddings_subset.p\", \"rb\"))\n",
    "len(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c092b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## implementing cosine similarity\n",
    "\n",
    "def cosine_similarity(A, B):\n",
    "    '''\n",
    "    Input:\n",
    "        A: a numpy array to a word vector\n",
    "        B: A numpy array to a word vector\n",
    "    Output:\n",
    "        cos: number that is cosine similarity between A and B.\n",
    "    '''    \n",
    "    dot = np.dot(A,B)\n",
    "    norma = np.sqrt(np.dot(A,A))\n",
    "    normb = np.sqrt(np.dot(B,B))\n",
    "    cos = dot / (norma*normb)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0a6298",
   "metadata": {},
   "outputs": [],
   "source": [
    "## implementing euclidean distance\n",
    "\n",
    "def euclidean(A, B):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        A: a numpy array to a word vector\n",
    "        B: A numpy array to a word vector\n",
    "    Output:\n",
    "        d: number representing the Euclidean distance between A and B.\n",
    "    \"\"\"\n",
    "\n",
    "    d = np.linalg.norm(A-B)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1803e16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## finding country of each capital\n",
    "\n",
    "def get_country(city1, country1, city2, embeddings):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        city1:the capital city of country1\n",
    "        country1: the country of capital1\n",
    "        city2: the capital city of country2\n",
    "        embeddings: a dictionary where the keys are words and values are their embeddings\n",
    "    Output:\n",
    "        countries: a dictionary with the most likely country and similarity score\n",
    "    \"\"\"\n",
    "    \n",
    "    group = set((city1, country1, city2))\n",
    "    \n",
    "    city1_emb = word_embeddings[city1] \n",
    "    \n",
    "    country1_emb =  word_embeddings[country1]\n",
    "    \n",
    "    city2_emb = word_embeddings[city2]\n",
    "    \n",
    "    vec = country1_emb - city1_emb + city2_emb\n",
    "    \n",
    "    similarity = -1      # initialise similarity\n",
    "    \n",
    "    for word in embedding.keys():\n",
    "        \n",
    "        if word not in group:\n",
    "            \n",
    "            word_emb = word_embeddings[word]\n",
    "            \n",
    "            cur_similarity = cosine_similarity(vec,word_emb)\n",
    "            \n",
    "            if cur_similarity > similarity:\n",
    "                \n",
    "                similarity = cur_similarity\n",
    "            \n",
    "                country = (word, similarity)\n",
    "\n",
    "    return country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ef1994",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_country('Athens', 'Greece', 'Cairo', word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2042b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking model accuracy\n",
    "\n",
    "def get_accuracy(word_embeddings, data):\n",
    "    '''\n",
    "    Input:\n",
    "        word_embeddings: a dictionary where the key is a word and the value is its embedding\n",
    "        data: a pandas dataframe containing all the country and capital city pairs\n",
    "    \n",
    "    Output:\n",
    "        accuracy: the accuracy of the model\n",
    "    '''\n",
    "    \n",
    "    num_correct = 0\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        \n",
    "        city1 = row['city1']\n",
    "        \n",
    "        country1 = row['country1']\n",
    "\n",
    "        city2 =  row['city2']\n",
    "\n",
    "        country2 = row['country2']\n",
    "        \n",
    "        predicted_country2, _ = get_country(city1,country1,city2,word_embeddings)\n",
    "        \n",
    "        if predicted_country2 == country2:\n",
    "            \n",
    "            num_correct += 1\n",
    "            \n",
    "    m = len(data) # getting number of rows\n",
    "    \n",
    "    accuracy = num_correct/m\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d615d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = get_accuracy(word_embeddings, data)\n",
    "print(f\"Accuracy is {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c91251",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting vectors using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4d5751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pca(X, n_components = 2):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        X: of dimension (m,n) where each row corresponds to a word vector\n",
    "        n_components: Number of components you want to keep.\n",
    "    Output:\n",
    "        X_reduced: data transformed in 2 dims/columns + regenerated original data\n",
    "    \"\"\"\n",
    "    X_demeaned = X - np.mean(X, axis = 0)      # mean center the data\n",
    "    \n",
    "    covariance_matrix = np.cov(X_demeaned, rowvar=False)   # calculate the covariance matrix\n",
    "    \n",
    "    eigen_vals, eigen_vecs =  np.linalg.eigh(covariance_matrix, UPLO = 'L') # calculate eigenvectors,eienvalues of the covariance matrix\n",
    "    \n",
    "    idx_sorted = np.argsort(eigen_vals)       # sort eigenvalue in increasing order\n",
    "    \n",
    "    idx_sorted_decreasing = idx_sorted[::-1] # reverse from highest to lowest\n",
    "    \n",
    "    eigen_vals_sorted = eigen_vals[idx_sorted_decreasing]\n",
    "    \n",
    "    eigen_vecs_sorted = eigen_vecs[:,idx_sorted_decreasing] # sort eigenvectors using the idx_sorted_decreasing indices\n",
    "    \n",
    "    eigen_vecs_subset = eigen_vecs_sorted[:, 0:n_components]\n",
    "    \n",
    "    # transform data by multiplying transpose of eigen vectors with transpose of demeaned data\n",
    "    \n",
    "    X_reduced = np.dot(eigen_vecs_subset.transpose(), X_demeaned.transpose()).transpose()\n",
    "    \n",
    "    return X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504ec811",
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing on words\n",
    "\n",
    "words = ['oil', 'gas', 'happy', 'sad', 'city', 'town',\n",
    "         'village', 'country', 'continent', 'petroleum', 'joyful']\n",
    "\n",
    "X = get_vectors(word_embeddings, words)\n",
    "\n",
    "# plotting\n",
    "\n",
    "result = compute_pca(X, 2)\n",
    "\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy =(result[i, 0] - 0.05, result[i, 1] + 0.1))\n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
